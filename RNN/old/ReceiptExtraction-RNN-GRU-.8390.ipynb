{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import LSTM, GRU, Bidirectional, SimpleRNN, RNN\n",
    "from keras.models import Model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this finds our json files\n",
    "path_to_json = 'CORD/train/json/'\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "\n",
    "# here I define my pandas Dataframe with the columns I want to get from the json\n",
    "jsons_data = pd.DataFrame(columns=['validline'])\n",
    "\n",
    "f = open(\"receipt_words.csv\", \"w\")\n",
    "print(\"filename\",\",\",\"index\",\",\",\"x1\",\",\",'x2',\",\",'x3',\",\",'x4',\",\",'y1',\",\",'y2',\",\",'y3',\",\",'y4',\",\",'is_key',\",\",'row_id',\",\",'text',\",\",'category',\",\",\"group_id\",file=f)\n",
    "\n",
    "\n",
    "# we need both the json and an index number so use enumerate()\n",
    "for index, js in enumerate(json_files):\n",
    "    with open(os.path.join(path_to_json, js)) as json_file:\n",
    "        json_text = json.load(json_file)\n",
    "        \n",
    "       # print(json_text)\n",
    "\n",
    "        # here you need to know the layout of your json and each json has to have\n",
    "        # the same structure (obviously not the structure I have here)\n",
    "        lines = json_text['valid_line']\n",
    "        \n",
    "        for x in lines:\n",
    "            words = x['words']\n",
    "            category = x['category']\n",
    "            group_id = x['group_id']\n",
    "            for y in words:\n",
    "                print(js,index,y['quad']['x1'],y['quad']['x2'],y['quad']['x3'],y['quad']['x4'],y['quad']['y1'],y['quad']['y2'],y['quad']['y3'],y['quad']['y4'],y['is_key'],y['row_id'],\"\\\"\"+y['text']+\"\\\"\",category,group_id,file=f,sep=\",\")\n",
    "\n",
    "       # city = json_text['features'][0]['properties']['name']\n",
    "        #lonlat = json_text['features'][0]['geometry']['coordinates']\n",
    "        # here I push a list of data into a pandas DataFrame at row given by 'index'\n",
    "        #f.close()\n",
    "        #jsons_data.loc[index] = [words, city, lonlat]\n",
    "\n",
    "# now that we have the pertinent json data in our DataFrame let's look at it\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "receipts_df = pd.read_csv(\"receipt_words.csv\")\n",
    "receipts_df.columns = receipts_df.columns.str.strip()\n",
    "receipts_df['text'] = receipts_df['text'].str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {\"START\":0,\"END\":1,\"a\":2,\"b\":3,\"c\":4,\"d\":5,\"e\":6,\"f\":7,\"g\":8,\"h\":9,\"i\":10,\"j\":11,\"k\":12,\"l\":13,\"m\":14,\"n\":15,\"o\":16,\"p\":17,\"q\":18,\"r\":19,\"s\":20,\"t\":21,\"u\":22,\"v\":23,\"w\":24,\"x\":25,\"y\":26,\"z\":27,\"0\":28,\"1\":29,\"2\":30,\"3\":31,\"4\":32,\"5\":33,\"6\":34,\"7\":35,\"8\":36,\"9\":37,\".\":38,\"-\":39,\",\":40,\":\":41,\" \":42}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p_test.SentimentText=p_test.SentimentText.astype(str)\n",
    "receipts_df.text = receipts_df.text.astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = receipts_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_words = []\n",
    "j=43\n",
    "for word in words:\n",
    "    indexed_word = []\n",
    "    #print(word)\n",
    "    word = word.lower();\n",
    "    for i in range(len(word)):\n",
    "        token = word[i]\n",
    "        if token not in char2idx:\n",
    "            char2idx[token] = j\n",
    "            j += 1\n",
    "        indexed_word.append(char2idx[token])\n",
    "    indexed_words.append(indexed_word)\n",
    "\n",
    "receipts_df[\"indexed_words\"] = indexed_words;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_bigram_probs(words, V, start_idx, end_idx, smoothing=1):\n",
    "  # structure of bigram probability matrix will be:\n",
    "  # (last word, current word) --> probability\n",
    "  # we will use add-1 smoothing\n",
    "  # note: we'll always ignore this from the END token\n",
    "  bigram_probs = np.ones((V, V)) * smoothing\n",
    "  for word in words:\n",
    "    for i in range(len(word)):\n",
    "      \n",
    "      if i == 0:\n",
    "        # beginning word\n",
    "        word[i]\n",
    "        bigram_probs[start_idx, word[i]] += 1\n",
    "      else:\n",
    "        # middle word\n",
    "        bigram_probs[word[i-1], word[i]] += 1\n",
    "\n",
    "      # if we're at the final word\n",
    "      # we update the bigram for last -> current\n",
    "      # AND current -> END token\n",
    "      if i == len(word) - 1:\n",
    "        # final word\n",
    "        bigram_probs[word[i], end_idx] += 1\n",
    "\n",
    "  # normalize the counts along the rows to get probabilities\n",
    "  bigram_probs /= bigram_probs.sum(axis=1, keepdims=True)\n",
    "  return bigram_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx = char2idx['START']\n",
    "end_idx = char2idx['END']\n",
    "V = len(char2idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_probs = get_bigram_probs(indexed_words, V, start_idx, end_idx, smoothing=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    " # a function to calculate normalized log prob score\n",
    "  # for a sentence\n",
    "def get_score(word):\n",
    "    #print(\"word\")\n",
    "    #print(word)\n",
    "    score = 0\n",
    "    if len(word) > 0 :\n",
    "        for i in range(len(word)):\n",
    "          if i == 0:\n",
    "            # beginning word\n",
    "            score += np.log(bigram_probs[start_idx, word[i]])\n",
    "          else:\n",
    "            # middle word\n",
    "            score += np.log(bigram_probs[word[i-1], word[i]])\n",
    "        # final word\n",
    "        score += np.log(bigram_probs[word[i-1], end_idx])\n",
    "\n",
    "    # normalize the score\n",
    "    # return score / (len(word) + 1)\n",
    "    if score == 0:\n",
    "        return score;\n",
    "    else :\n",
    "        return  ((-1)*(len(word) + 1)/score)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from future.utils import iteritems\n",
    "\n",
    "idx2char = dict((v, k) for k, v in iteritems(char2idx))\n",
    "def get_word(word):\n",
    "    return ''.join(idx2char[i] for i in word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "receipts_df['text_score'] = receipts_df.apply(lambda row : get_score(row['indexed_words']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = receipts_df['category'] \n",
    "category2idx = {}\n",
    "idx2category = {}\n",
    "j=0;\n",
    "for category in categories:\n",
    "    if category not in category2idx:\n",
    "        category2idx[category] = j\n",
    "        idx2category[j] = category\n",
    "        j+=1\n",
    "\n",
    "category2idx\n",
    "\n",
    "def get_index_for_category(category):\n",
    "    return category2idx[category]\n",
    "\n",
    "\n",
    "def get_category_for_index(index):\n",
    "    return idx2category[index]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "receipts_df['category_idx'] = receipts_df.apply(lambda row : get_index_for_category(row['category']), axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = receipts_df[['row_id','text_score','category_idx','index']].sort_values([\"index\", \"row_id\"], ascending = (True, True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18958, 4)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = []\n",
    "Y_1 = []\n",
    "tmp_feature = []\n",
    "tmp_output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prevIndx = -1;\n",
    "for ind in data_1.index: \n",
    "    #print (data_1['index'][ind])\n",
    "    #print (prevIndx)\n",
    "    if ( data_1['index'][ind] != prevIndx):\n",
    "        prevIndx = data_1['index'][ind]\n",
    "        tmp_feature = []\n",
    "        tmp_output = []\n",
    "        if(prevIndx !=  -1):\n",
    "            X_1.append(tmp_feature)\n",
    "            Y_1.append(tmp_output)\n",
    "    tmp_feature.append(data_1['text_score'][ind])\n",
    "    tmp_output.append(data_1['text_score'][ind])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "787"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "787"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 sentences have disparate input-output lengths.\n"
     ]
    }
   ],
   "source": [
    "different_length = [1 if len(input) != len(output) else 0 for input, output in zip(X_1, Y_1)]\n",
    "print(\"{} sentences have disparate input-output lengths.\".format(sum(different_length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of longest sentence: 135\n"
     ]
    }
   ],
   "source": [
    "lengths = [len(seq) for seq in X_1]\n",
    "print(\"Length of longest sentence: {}\".format(max(lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "sns.boxplot(lengths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_SEQ_LENGTH = max(lengths)  # sequences greater than 100 in length will be truncated\n",
    "\n",
    "X_padded = pad_sequences(X_1, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
    "Y_padded = pad_sequences(Y_1, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(787, 135)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(787, 135)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y_padded = to_categorical(Y_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = X_padded, Y_padded\n",
    "VALID_SIZE = 0.15\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X_padded, Y_padded, test_size=VALID_SIZE, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(787, 135, 96)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# create architecture\n",
    "VOCABULARY_SIZE = 135\n",
    "EMBEDDING_SIZE = 135\n",
    "MAX_SEQ_LENGTH = 135\n",
    "NUM_CLASSES = Y.shape[2]\n",
    "rnn_model = Sequential()\n",
    "\n",
    "# create embedding layer - usually the first layer in text problems\n",
    "rnn_model.add(Embedding(input_dim     =  VOCABULARY_SIZE,         # vocabulary size - number of unique words in data\n",
    "                        output_dim    =  EMBEDDING_SIZE,          # length of vector with which each word is represented\n",
    "                        input_length  =  MAX_SEQ_LENGTH,          # length of input sequence\n",
    "                        trainable     =  False                    # False - don't update the embeddings\n",
    "))\n",
    "\n",
    "# add an RNN layer which contains 64 RNN cells\n",
    "rnn_model.add(GRU(64, \n",
    "              return_sequences=True  # True - return whole sequence; False - return single output of the end of the sequence\n",
    "))\n",
    "\n",
    "# add time distributed (output at each sequence) layer\n",
    "rnn_model.add(TimeDistributed(Dense(NUM_CLASSES, activation='softmax')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 135, 135)          18225     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 135, 64)           38400     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 135, 96)           6240      \n",
      "=================================================================\n",
      "Total params: 62,865\n",
      "Trainable params: 44,640\n",
      "Non-trainable params: 18,225\n",
      "_________________________________________________________________\n",
      "Train on 668 samples, validate on 119 samples\n",
      "Epoch 1/10\n",
      "668/668 [==============================] - 19s 29ms/step - loss: 2.6940 - acc: 0.8017 - val_loss: 1.1147 - val_acc: 0.8216\n",
      "Epoch 2/10\n",
      "668/668 [==============================] - 17s 25ms/step - loss: 0.9610 - acc: 0.8216 - val_loss: 0.8928 - val_acc: 0.8216\n",
      "Epoch 3/10\n",
      "668/668 [==============================] - 18s 26ms/step - loss: 0.8659 - acc: 0.8216 - val_loss: 0.8458 - val_acc: 0.8216\n",
      "Epoch 4/10\n",
      "668/668 [==============================] - 18s 27ms/step - loss: 0.8294 - acc: 0.8216 - val_loss: 0.8163 - val_acc: 0.8216\n",
      "Epoch 5/10\n",
      "668/668 [==============================] - 18s 27ms/step - loss: 0.8020 - acc: 0.8216 - val_loss: 0.7912 - val_acc: 0.8216\n",
      "Epoch 6/10\n",
      "668/668 [==============================] - 18s 27ms/step - loss: 0.7777 - acc: 0.8216 - val_loss: 0.7676 - val_acc: 0.8216\n",
      "Epoch 7/10\n",
      "668/668 [==============================] - 21s 31ms/step - loss: 0.7535 - acc: 0.8216 - val_loss: 0.7436 - val_acc: 0.8216\n",
      "Epoch 8/10\n",
      " 32/668 [>.............................] - ETA: 20s - loss: 0.7020 - acc: 0.8317"
     ]
    }
   ],
   "source": [
    "rnn_model.compile(loss      =  'categorical_crossentropy',\n",
    "                  optimizer =  'adam',\n",
    "                  metrics   =  ['acc'])\n",
    "rnn_model.summary()\n",
    "rnn_training = rnn_model.fit(X_train, Y_train, batch_size=16, epochs=10, validation_data=(X_validation, Y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise training history\n",
    "plt.plot(rnn_training.history['acc'])\n",
    "plt.plot(rnn_training.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
