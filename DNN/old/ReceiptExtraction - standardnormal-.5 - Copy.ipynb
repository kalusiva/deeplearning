{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "\n",
    "# this finds our json files\n",
    "path_to_json = 'F:/kalu/BITS/DeepLearning/assignment1/CORD/train/json/'\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "\n",
    "# here I define my pandas Dataframe with the columns I want to get from the json\n",
    "jsons_data = pd.DataFrame(columns=['validline'])\n",
    "\n",
    "f = open(\"F:/kalu/BITS/DeepLearning/assignment1/receipt_words.csv\", \"a\")\n",
    "print(\"filename\",\",\",\"index\",\",\",\"x1\",\",\",'x2',\",\",'x3',\",\",'x4',\",\",'y1',\",\",'y2',\",\",'y3',\",\",'y4',\",\",'is_key',\",\",'row_id',\",\",'text',\",\",'category',\",\",\"group_id\",file=f)\n",
    "\n",
    "\n",
    "# we need both the json and an index number so use enumerate()\n",
    "for index, js in enumerate(json_files):\n",
    "    with open(os.path.join(path_to_json, js)) as json_file:\n",
    "        json_text = json.load(json_file)\n",
    "        \n",
    "       # print(json_text)\n",
    "\n",
    "        # here you need to know the layout of your json and each json has to have\n",
    "        # the same structure (obviously not the structure I have here)\n",
    "        lines = json_text['valid_line']\n",
    "        \n",
    "        for x in lines:\n",
    "            words = x['words']\n",
    "            category = x['category']\n",
    "            group_id = x['group_id']\n",
    "            for y in words:\n",
    "                print(js,index,y['quad']['x1'],y['quad']['x2'],y['quad']['x3'],y['quad']['x4'],y['quad']['y1'],y['quad']['y2'],y['quad']['y3'],y['quad']['y4'],y['is_key'],y['row_id'],\"\\\"\"+y['text']+\"\\\"\",category,group_id,file=f,sep=\",\")\n",
    "\n",
    "       # city = json_text['features'][0]['properties']['name']\n",
    "        #lonlat = json_text['features'][0]['geometry']['coordinates']\n",
    "        # here I push a list of data into a pandas DataFrame at row given by 'index'\n",
    "        #f.close()\n",
    "        #jsons_data.loc[index] = [words, city, lonlat]\n",
    "\n",
    "# now that we have the pertinent json data in our DataFrame let's look at it\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "receipts_df = pd.read_csv(\"F:/kalu/BITS/DeepLearning/assignment1/receipt_words.csv\")\n",
    "receipts_df.columns = receipts_df.columns.str.strip()\n",
    "receipts_df['text'] = receipts_df['text'].str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filename    object\n",
       "index        int64\n",
       "x1           int64\n",
       "x2           int64\n",
       "x3           int64\n",
       "x4           int64\n",
       "y1           int64\n",
       "y2           int64\n",
       "y3           int64\n",
       "y4           int64\n",
       "is_key       int64\n",
       "row_id       int64\n",
       "text        object\n",
       "category    object\n",
       "group_id     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "receipts_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {\"START\":0,\"END\":1,\"a\":2,\"b\":3,\"c\":4,\"d\":5,\"e\":6,\"f\":7,\"g\":8,\"h\":9,\"i\":10,\"j\":11,\"k\":12,\"l\":13,\"m\":14,\"n\":15,\"o\":16,\"p\":17,\"q\":18,\"r\":19,\"s\":20,\"t\":21,\"u\":22,\"v\":23,\"w\":24,\"x\":25,\"y\":26,\"z\":27,\"0\":28,\"1\":29,\"2\":30,\"3\":31,\"4\":32,\"5\":33,\"6\":34,\"7\":35,\"8\":36,\"9\":37,\".\":38,\"-\":39,\",\":40,\":\":41,\" \":42}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filename    object\n",
       "index        int64\n",
       "x1           int64\n",
       "x2           int64\n",
       "x3           int64\n",
       "x4           int64\n",
       "y1           int64\n",
       "y2           int64\n",
       "y3           int64\n",
       "y4           int64\n",
       "is_key       int64\n",
       "row_id       int64\n",
       "text        object\n",
       "category    object\n",
       "group_id     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#p_test.SentimentText=p_test.SentimentText.astype(str)\n",
    "receipts_df.text = receipts_df.text.astype(str)\n",
    "receipts_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = receipts_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_words = []\n",
    "j=43\n",
    "for word in words:\n",
    "    indexed_word = []\n",
    "    #print(word)\n",
    "    word = word.lower();\n",
    "    for i in range(len(word)):\n",
    "        token = word[i]\n",
    "        if token not in char2idx:\n",
    "            char2idx[token] = j\n",
    "            j += 1\n",
    "        indexed_word.append(char2idx[token])\n",
    "    indexed_words.append(indexed_word)\n",
    "\n",
    "receipts_df[\"indexed_words\"] = indexed_words;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_bigram_probs(words, V, start_idx, end_idx, smoothing=1):\n",
    "  # structure of bigram probability matrix will be:\n",
    "  # (last word, current word) --> probability\n",
    "  # we will use add-1 smoothing\n",
    "  # note: we'll always ignore this from the END token\n",
    "  bigram_probs = np.ones((V, V)) * smoothing\n",
    "  for word in words:\n",
    "    for i in range(len(word)):\n",
    "      \n",
    "      if i == 0:\n",
    "        # beginning word\n",
    "        word[i]\n",
    "        bigram_probs[start_idx, word[i]] += 1\n",
    "      else:\n",
    "        # middle word\n",
    "        bigram_probs[word[i-1], word[i]] += 1\n",
    "\n",
    "      # if we're at the final word\n",
    "      # we update the bigram for last -> current\n",
    "      # AND current -> END token\n",
    "      if i == len(word) - 1:\n",
    "        # final word\n",
    "        bigram_probs[word[i], end_idx] += 1\n",
    "\n",
    "  # normalize the counts along the rows to get probabilities\n",
    "  bigram_probs /= bigram_probs.sum(axis=1, keepdims=True)\n",
    "  return bigram_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx = char2idx['START']\n",
    "end_idx = char2idx['END']\n",
    "V = len(char2idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_probs = get_bigram_probs(indexed_words, V, start_idx, end_idx, smoothing=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.27301010e-06, 5.27301010e-06, 1.57715732e-02, ...,\n",
       "        5.27301010e-06, 1.10733212e-04, 1.10733212e-04],\n",
       "       [1.53846154e-02, 1.53846154e-02, 1.53846154e-02, ...,\n",
       "        1.53846154e-02, 1.53846154e-02, 1.53846154e-02],\n",
       "       [1.57915515e-05, 7.97631267e-02, 8.05369128e-04, ...,\n",
       "        1.57915515e-05, 1.57915515e-05, 1.57915515e-05],\n",
       "       ...,\n",
       "       [1.17647059e-02, 1.17647059e-02, 1.17647059e-02, ...,\n",
       "        1.17647059e-02, 1.17647059e-02, 1.17647059e-02],\n",
       "       [1.17647059e-02, 2.47058824e-01, 1.17647059e-02, ...,\n",
       "        1.17647059e-02, 1.17647059e-02, 1.17647059e-02],\n",
       "       [1.17647059e-02, 1.17647059e-02, 1.29411765e-01, ...,\n",
       "        1.17647059e-02, 1.17647059e-02, 1.17647059e-02]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    " # a function to calculate normalized log prob score\n",
    "  # for a sentence\n",
    "def get_score(word):\n",
    "    #print(\"word\")\n",
    "    #print(word)\n",
    "    score = 0\n",
    "    if len(word) > 0 :\n",
    "        for i in range(len(word)):\n",
    "          if i == 0:\n",
    "            # beginning word\n",
    "            score += np.log(bigram_probs[start_idx, word[i]])\n",
    "          else:\n",
    "            # middle word\n",
    "            score += np.log(bigram_probs[word[i-1], word[i]])\n",
    "        # final word\n",
    "        score += np.log(bigram_probs[word[i-1], end_idx])\n",
    "\n",
    "    # normalize the score\n",
    "    # return score / (len(word) + 1)\n",
    "    if score == 0:\n",
    "        return score;\n",
    "    else :\n",
    "        return  ((-1)*(len(word) + 1)/score)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from future.utils import iteritems\n",
    "\n",
    "idx2char = dict((v, k) for k, v in iteritems(char2idx))\n",
    "def get_word(word):\n",
    "    return ''.join(idx2char[i] for i in word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REAL: ku SCORE: 36.43195142778534\n",
      "FAKE: ed SCORE: 32.48651424748004\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your own sentence:\n",
      " test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 44.341744807474406\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue? [Y/n] n\n"
     ]
    }
   ],
   "source": [
    "# when we sample a fake sentence, we want to ensure not to sample\n",
    "  # start token or end token\n",
    "sample_probs = np.ones(V)\n",
    "sample_probs[start_idx] = 0\n",
    "sample_probs[end_idx] = 0\n",
    "sample_probs /= sample_probs.sum()\n",
    " # test our model on real and fake sentences\n",
    "while True:\n",
    "    # real sentence\n",
    "    real_idx = np.random.choice(len(indexed_words))\n",
    "    real = indexed_words[real_idx]\n",
    "    # fake sentence\n",
    "    fake = np.random.choice(V, size=len(real), p=sample_probs)\n",
    "\n",
    "    print(\"REAL:\", get_word(real), \"SCORE:\", get_score(real))\n",
    "    print(\"FAKE:\", get_word(fake), \"SCORE:\", get_score(fake))\n",
    "\n",
    "    # input your own sentence\n",
    "    custom = input(\"Enter your own sentence:\\n\")\n",
    "    #custom = custom.lower().split()\n",
    "    custom\n",
    "    # check that all tokens exist in word2idx (otherwise, we can't get score)\n",
    "    bad_sentence = False\n",
    "    for i in range(len(custom)):\n",
    "        token = custom[i]\n",
    "        #print(\"custom token i\")\n",
    "        #print(token)\n",
    "        if token not in char2idx:\n",
    "            bad_sentence = True\n",
    "\n",
    "    if bad_sentence:\n",
    "      print(\"Sorry, you entered words that are not in the vocabulary\")\n",
    "    else:\n",
    "      # convert sentence into list of indexes\n",
    "      custom = [char2idx[token] for token in custom]\n",
    "      print(\"SCORE:\", get_score(custom))\n",
    "\n",
    "\n",
    "    cont = input(\"Continue? [Y/n]\")\n",
    "    if cont and cont.lower() in ('N', 'n'):\n",
    "      break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "receipts_df['text_score'] = receipts_df.apply(lambda row : get_score(row['indexed_words']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>index</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>y1</th>\n",
       "      <th>y2</th>\n",
       "      <th>y3</th>\n",
       "      <th>y4</th>\n",
       "      <th>is_key</th>\n",
       "      <th>row_id</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>group_id</th>\n",
       "      <th>indexed_words</th>\n",
       "      <th>text_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>receipt_00000.json</td>\n",
       "      <td>0</td>\n",
       "      <td>256</td>\n",
       "      <td>270</td>\n",
       "      <td>270</td>\n",
       "      <td>256</td>\n",
       "      <td>374</td>\n",
       "      <td>374</td>\n",
       "      <td>390</td>\n",
       "      <td>390</td>\n",
       "      <td>0</td>\n",
       "      <td>2179893</td>\n",
       "      <td>x</td>\n",
       "      <td>menu.cnt</td>\n",
       "      <td>3</td>\n",
       "      <td>[25]</td>\n",
       "      <td>39.091172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>receipt_00000.json</td>\n",
       "      <td>0</td>\n",
       "      <td>258</td>\n",
       "      <td>270</td>\n",
       "      <td>270</td>\n",
       "      <td>258</td>\n",
       "      <td>402</td>\n",
       "      <td>402</td>\n",
       "      <td>418</td>\n",
       "      <td>418</td>\n",
       "      <td>0</td>\n",
       "      <td>2179894</td>\n",
       "      <td>x</td>\n",
       "      <td>menu.cnt</td>\n",
       "      <td>4</td>\n",
       "      <td>[25]</td>\n",
       "      <td>39.091172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>receipt_00000.json</td>\n",
       "      <td>0</td>\n",
       "      <td>258</td>\n",
       "      <td>272</td>\n",
       "      <td>272</td>\n",
       "      <td>258</td>\n",
       "      <td>428</td>\n",
       "      <td>428</td>\n",
       "      <td>444</td>\n",
       "      <td>444</td>\n",
       "      <td>0</td>\n",
       "      <td>2179895</td>\n",
       "      <td>x</td>\n",
       "      <td>menu.cnt</td>\n",
       "      <td>5</td>\n",
       "      <td>[25]</td>\n",
       "      <td>39.091172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>receipt_00000.json</td>\n",
       "      <td>0</td>\n",
       "      <td>260</td>\n",
       "      <td>274</td>\n",
       "      <td>274</td>\n",
       "      <td>260</td>\n",
       "      <td>456</td>\n",
       "      <td>456</td>\n",
       "      <td>470</td>\n",
       "      <td>470</td>\n",
       "      <td>0</td>\n",
       "      <td>2179896</td>\n",
       "      <td>x</td>\n",
       "      <td>menu.cnt</td>\n",
       "      <td>6</td>\n",
       "      <td>[25]</td>\n",
       "      <td>39.091172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>receipt_00000.json</td>\n",
       "      <td>0</td>\n",
       "      <td>258</td>\n",
       "      <td>274</td>\n",
       "      <td>274</td>\n",
       "      <td>258</td>\n",
       "      <td>480</td>\n",
       "      <td>480</td>\n",
       "      <td>496</td>\n",
       "      <td>496</td>\n",
       "      <td>0</td>\n",
       "      <td>2179897</td>\n",
       "      <td>x</td>\n",
       "      <td>menu.cnt</td>\n",
       "      <td>7</td>\n",
       "      <td>[25]</td>\n",
       "      <td>39.091172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18953</th>\n",
       "      <td>receipt_00799.json</td>\n",
       "      <td>786</td>\n",
       "      <td>1002</td>\n",
       "      <td>1179</td>\n",
       "      <td>1180</td>\n",
       "      <td>1002</td>\n",
       "      <td>1465</td>\n",
       "      <td>1465</td>\n",
       "      <td>1531</td>\n",
       "      <td>1528</td>\n",
       "      <td>0</td>\n",
       "      <td>678299</td>\n",
       "      <td>18.000</td>\n",
       "      <td>total.total_price</td>\n",
       "      <td>4</td>\n",
       "      <td>[29, 36, 38, 28, 28, 28]</td>\n",
       "      <td>73.278820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18954</th>\n",
       "      <td>receipt_00799.json</td>\n",
       "      <td>786</td>\n",
       "      <td>274</td>\n",
       "      <td>415</td>\n",
       "      <td>414</td>\n",
       "      <td>274</td>\n",
       "      <td>1616</td>\n",
       "      <td>1618</td>\n",
       "      <td>1685</td>\n",
       "      <td>1683</td>\n",
       "      <td>1</td>\n",
       "      <td>678300</td>\n",
       "      <td>Tunai</td>\n",
       "      <td>total.cashprice</td>\n",
       "      <td>4</td>\n",
       "      <td>[21, 22, 15, 2, 10]</td>\n",
       "      <td>35.914549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18955</th>\n",
       "      <td>receipt_00799.json</td>\n",
       "      <td>786</td>\n",
       "      <td>1002</td>\n",
       "      <td>1173</td>\n",
       "      <td>1176</td>\n",
       "      <td>1002</td>\n",
       "      <td>1621</td>\n",
       "      <td>1621</td>\n",
       "      <td>1685</td>\n",
       "      <td>1686</td>\n",
       "      <td>0</td>\n",
       "      <td>678300</td>\n",
       "      <td>19.000</td>\n",
       "      <td>total.cashprice</td>\n",
       "      <td>4</td>\n",
       "      <td>[29, 37, 38, 28, 28, 28]</td>\n",
       "      <td>64.601556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18956</th>\n",
       "      <td>receipt_00799.json</td>\n",
       "      <td>786</td>\n",
       "      <td>268</td>\n",
       "      <td>474</td>\n",
       "      <td>475</td>\n",
       "      <td>268</td>\n",
       "      <td>1693</td>\n",
       "      <td>1695</td>\n",
       "      <td>1762</td>\n",
       "      <td>1761</td>\n",
       "      <td>1</td>\n",
       "      <td>678301</td>\n",
       "      <td>Kembali</td>\n",
       "      <td>total.changeprice</td>\n",
       "      <td>4</td>\n",
       "      <td>[12, 6, 14, 3, 2, 13, 10]</td>\n",
       "      <td>49.430316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18957</th>\n",
       "      <td>receipt_00799.json</td>\n",
       "      <td>786</td>\n",
       "      <td>1031</td>\n",
       "      <td>1177</td>\n",
       "      <td>1177</td>\n",
       "      <td>1031</td>\n",
       "      <td>1699</td>\n",
       "      <td>1699</td>\n",
       "      <td>1765</td>\n",
       "      <td>1765</td>\n",
       "      <td>0</td>\n",
       "      <td>678301</td>\n",
       "      <td>1.000</td>\n",
       "      <td>total.changeprice</td>\n",
       "      <td>4</td>\n",
       "      <td>[29, 38, 28, 28, 28]</td>\n",
       "      <td>73.770362</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18958 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 filename  index    x1    x2    x3    x4    y1    y2    y3  \\\n",
       "0      receipt_00000.json      0   256   270   270   256   374   374   390   \n",
       "1      receipt_00000.json      0   258   270   270   258   402   402   418   \n",
       "2      receipt_00000.json      0   258   272   272   258   428   428   444   \n",
       "3      receipt_00000.json      0   260   274   274   260   456   456   470   \n",
       "4      receipt_00000.json      0   258   274   274   258   480   480   496   \n",
       "...                   ...    ...   ...   ...   ...   ...   ...   ...   ...   \n",
       "18953  receipt_00799.json    786  1002  1179  1180  1002  1465  1465  1531   \n",
       "18954  receipt_00799.json    786   274   415   414   274  1616  1618  1685   \n",
       "18955  receipt_00799.json    786  1002  1173  1176  1002  1621  1621  1685   \n",
       "18956  receipt_00799.json    786   268   474   475   268  1693  1695  1762   \n",
       "18957  receipt_00799.json    786  1031  1177  1177  1031  1699  1699  1765   \n",
       "\n",
       "         y4  is_key   row_id     text           category  group_id  \\\n",
       "0       390       0  2179893        x           menu.cnt         3   \n",
       "1       418       0  2179894        x           menu.cnt         4   \n",
       "2       444       0  2179895        x           menu.cnt         5   \n",
       "3       470       0  2179896        x           menu.cnt         6   \n",
       "4       496       0  2179897        x           menu.cnt         7   \n",
       "...     ...     ...      ...      ...                ...       ...   \n",
       "18953  1528       0   678299   18.000  total.total_price         4   \n",
       "18954  1683       1   678300    Tunai    total.cashprice         4   \n",
       "18955  1686       0   678300   19.000    total.cashprice         4   \n",
       "18956  1761       1   678301  Kembali  total.changeprice         4   \n",
       "18957  1765       0   678301    1.000  total.changeprice         4   \n",
       "\n",
       "                   indexed_words  text_score  \n",
       "0                           [25]   39.091172  \n",
       "1                           [25]   39.091172  \n",
       "2                           [25]   39.091172  \n",
       "3                           [25]   39.091172  \n",
       "4                           [25]   39.091172  \n",
       "...                          ...         ...  \n",
       "18953   [29, 36, 38, 28, 28, 28]   73.278820  \n",
       "18954        [21, 22, 15, 2, 10]   35.914549  \n",
       "18955   [29, 37, 38, 28, 28, 28]   64.601556  \n",
       "18956  [12, 6, 14, 3, 2, 13, 10]   49.430316  \n",
       "18957       [29, 38, 28, 28, 28]   73.770362  \n",
       "\n",
       "[18958 rows x 17 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "receipts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = receipts_df['category'] \n",
    "category2idx = {}\n",
    "idx2category = {}\n",
    "j=0;\n",
    "for category in categories:\n",
    "    if category not in category2idx:\n",
    "        category2idx[category] = j\n",
    "        idx2category[j] = category\n",
    "        j+=1\n",
    "\n",
    "category2idx\n",
    "\n",
    "def get_index_for_category(category):\n",
    "    return category2idx[category]\n",
    "\n",
    "\n",
    "def get_category_for_index(index):\n",
    "    return idx2category[index]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "receipts_df['category_idx'] = receipts_df.apply(lambda row : get_index_for_category(row['category']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(category2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 60)                720       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 30)                1830      \n",
      "=================================================================\n",
      "Total params: 2,550\n",
      "Trainable params: 2,550\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "3034/3034 [==============================] - 3s 870us/step - loss: 2.7889 - accuracy: 0.4420\n",
      "Epoch 2/25\n",
      "3034/3034 [==============================] - 3s 888us/step - loss: 2.5186 - accuracy: 0.4754\n",
      "Epoch 3/25\n",
      "3034/3034 [==============================] - 3s 862us/step - loss: 2.4801 - accuracy: 0.4853\n",
      "Epoch 4/25\n",
      "3034/3034 [==============================] - 3s 860us/step - loss: 2.4583 - accuracy: 0.4898\n",
      "Epoch 5/25\n",
      "3034/3034 [==============================] - 3s 860us/step - loss: 2.4437 - accuracy: 0.4947\n",
      "Epoch 6/25\n",
      "3034/3034 [==============================] - 3s 861us/step - loss: 2.4327 - accuracy: 0.4958\n",
      "Epoch 7/25\n",
      "3034/3034 [==============================] - 3s 866us/step - loss: 2.4228 - accuracy: 0.4977\n",
      "Epoch 8/25\n",
      "3034/3034 [==============================] - 3s 853us/step - loss: 2.4163 - accuracy: 0.4983\n",
      "Epoch 9/25\n",
      "3034/3034 [==============================] - 3s 852us/step - loss: 2.4083 - accuracy: 0.4993\n",
      "Epoch 10/25\n",
      "3034/3034 [==============================] - 3s 864us/step - loss: 2.4028 - accuracy: 0.5003\n",
      "Epoch 11/25\n",
      "3034/3034 [==============================] - 3s 862us/step - loss: 2.3989 - accuracy: 0.5035\n",
      "Epoch 12/25\n",
      "3034/3034 [==============================] - 3s 870us/step - loss: 2.3941 - accuracy: 0.5003\n",
      "Epoch 13/25\n",
      "3034/3034 [==============================] - 3s 862us/step - loss: 2.3889 - accuracy: 0.5016\n",
      "Epoch 14/25\n",
      "3034/3034 [==============================] - 3s 862us/step - loss: 2.3860 - accuracy: 0.5023\n",
      "Epoch 15/25\n",
      "3034/3034 [==============================] - 3s 857us/step - loss: 2.3814 - accuracy: 0.5012\n",
      "Epoch 16/25\n",
      "3034/3034 [==============================] - 3s 864us/step - loss: 2.3777 - accuracy: 0.5031\n",
      "Epoch 17/25\n",
      "3034/3034 [==============================] - 3s 857us/step - loss: 2.3753 - accuracy: 0.5035\n",
      "Epoch 18/25\n",
      "3034/3034 [==============================] - 3s 865us/step - loss: 2.3733 - accuracy: 0.5015\n",
      "Epoch 19/25\n",
      "3034/3034 [==============================] - 3s 883us/step - loss: 2.3689 - accuracy: 0.5048\n",
      "Epoch 20/25\n",
      "3034/3034 [==============================] - 3s 860us/step - loss: 2.3672 - accuracy: 0.5037\n",
      "Epoch 21/25\n",
      "3034/3034 [==============================] - 3s 853us/step - loss: 2.3640 - accuracy: 0.5042\n",
      "Epoch 22/25\n",
      "3034/3034 [==============================] - 3s 858us/step - loss: 2.3603 - accuracy: 0.5044\n",
      "Epoch 23/25\n",
      "3034/3034 [==============================] - 3s 859us/step - loss: 2.3594 - accuracy: 0.5050\n",
      "Epoch 24/25\n",
      "3034/3034 [==============================] - 3s 850us/step - loss: 2.3575 - accuracy: 0.5054\n",
      "Epoch 25/25\n",
      "3034/3034 [==============================] - 3s 856us/step - loss: 2.3527 - accuracy: 0.5032\n"
     ]
    }
   ],
   "source": [
    "data = receipts_df[['x1','x2','x3','x4','y1','y2','y3','y4','is_key','text_score','category_idx']]\n",
    "\n",
    "dnnModel = models.Sequential()\n",
    "# Layer 1 = input layer\n",
    "# specify the input size in the first layer.\n",
    "\n",
    "dnnModel.add(layers.Dense(60, activation='elu', input_shape= (10,)))\n",
    "\n",
    "# Layer 2 = hidden layer \n",
    "#dnnModel.add(layers.Dense(60, activation='relu'))\n",
    "\n",
    "# Layer 3 = hidden layer \n",
    "#dnnModel.add(layers.Dense(30, activation='linear'))\n",
    "\n",
    "# Layer 4 = output layer\n",
    "dnnModel.add(layers.Dense(len(category2idx), activation='sigmoid'))\n",
    "\n",
    "dnnModel.summary()\n",
    "dnnModel.compile( optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy'] )\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2)\n",
    "y = data.pop('category_idx')\n",
    "X = data\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "x = X.values #returns a numpy array\n",
    "#min_max_scaler = preprocessing.MinMaxScaler()\n",
    "min_max_scaler = preprocessing.StandardScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "X = pd.DataFrame(x_scaled)\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X.index,y.index,test_size=0.2)\n",
    "xtrain=X.iloc[X_train]\n",
    "xtest=X.iloc[X_test]\n",
    "y_train=y.iloc[y_train]\n",
    "y_test=y.iloc[y_test]\n",
    "ytrain = np.zeros((len(y_train), len(idx2category.values())))\n",
    "ytest = np.zeros((len(y_test), len(idx2category.values())))\n",
    "for i in range(len(y_train)):\n",
    "    #print(y_train.values[i])\n",
    "    ytrain[i][y_train.values[i]]=1\n",
    "    \n",
    "for i in range(len(y_test)):\n",
    "    #print(y_train.values[i])\n",
    "    ytrain[i][y_test.values[i]]=1\n",
    "\n",
    "xtrain=tf.convert_to_tensor(xtrain)\n",
    "\n",
    "ytraintens=tf.convert_to_tensor(ytrain)\n",
    "\n",
    "h  = dnnModel.fit( xtrain, ytraintens, epochs=25, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
